# Importing Libraries
import pandas as pd
import numpy as np
import matplsotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency
from fancyimpute import KNN 
import os
from sklearn.metrics import r2_score
from scipy import stats
%matplotlib inline

# check Working directory
os.getcwd()

'C:\\Users\\deekshitha.r'

# Load data
df = pd.read_excel("Absenteeism_at_work_Project.xls")

Exploratory data analysis

# Top 5 rows of data
df.head()

# data types of the given variables
df.dtypes

ID                                   int64
Reason for absence                 float64
Month of absence                   float64
Day of the week                      int64
Seasons                              int64
Transportation expense             float64
Distance from Residence to Work    float64
Service time                       float64
Age                                float64
Work load Average/day              float64
Hit target                         float64
Disciplinary failure               float64
Education                          float64
Son                                float64
Social drinker                     float64
Social smoker                      float64
Pet                                float64
Weight                             float64
Height                             float64
Body mass index                    float64
Absenteeism time in hours          float64
dtype: object


# No. of unique values present in varuables
df.nunique()

ID                                 36
Reason for absence                 28
Month of absence                   13
Day of the week                     5
Seasons                             4
Transportation expense             24
Distance from Residence to Work    25
Service time                       18
Age                                22
Work load Average/day              38
Hit target                         13
Disciplinary failure                2
Education                           4
Son                                 5
Social drinker                      2
Social smoker                       2
Pet                                 6
Weight                             26
Height                             14
Body mass index                    17
Absenteeism time in hours          19
dtype: int64

# Shape of our data
df.shape

# From the problem statement, our file has been categorising the variables in two category " Continuos" and "Categorical"
continuous_variables = ['Distance from Residence to Work', 'Service time', 'Age', 'Work load Average/day ', 'Transportation expense',
       'Hit target', 'Weight', 'Height', 'Body mass index', 'Absenteeism time in hours']

categorical_variables = ['ID','Reason for absence','Month of absence','Day of the week',
                     'Seasons','Disciplinary failure', 'Education', 'Social drinker',
                     'Social smoker', 'Pet', 'Son']


Missing Value Analysis

# Let's create a dataframe with missing values present in each variable
missing_val = pd.DataFrame(df.isnull().sum()).reset_index()

# Let's rename variables of missing_val dataframe
missing_val = missing_val.rename(columns = {'index': 'Variables', 0: 'Missing_percentage'})

# Let's calculate the percentage missing value
missing_val['Missing_percentage'] = (missing_val['Missing_percentage']/len(df))*100

# Let's sort the missing_val in Descending order
missing_val = missing_val.sort_values('Missing_percentage', ascending = False).reset_index(drop = True)

# Let's save output result into csv file
missing_val.to_csv("Missing_perc.csv", index = False)

Imputational Methods

# Let's Drop the observation in which "Absenteeism time in hours" has missing value
df = df.drop(df[df['Absenteeism time in hours'].isnull()].index, axis=0)
print(df.shape)
print(df['Absenteeism time in hours'].isnull().sum())

(718, 21)
0

#Apply KNN imputation algorithm
df = pd.DataFrame(KNN(k = 3).fit_transform(df), columns = df.columns)

Imputing row 1/718 with 0 missing, elapsed time: 0.686
Imputing row 101/718 with 1 missing, elapsed time: 0.718
Imputing row 201/718 with 0 missing, elapsed time: 0.733
Imputing row 301/718 with 0 missing, elapsed time: 0.733
Imputing row 401/718 with 0 missing, elapsed time: 0.733
Imputing row 501/718 with 1 missing, elapsed time: 0.733
Imputing row 601/718 with 0 missing, elapsed time: 0.733
Imputing row 701/718 with 0 missing, elapsed time: 0.733

Outlier Analysis

# Let us Plot BoxPlot # continuous variables
%matplotlib inline
plt.boxplot(df['Transportation expense'])
plt.xlabel("'Transportation expense'")
plt.title("BoxPlot of Variable 'Transportation expense'")
plt.ylabel('Values')

Text(0, 0.5, 'Values')

# Let us plot work load Average per day
plt.boxplot(df['Work load Average/day '])
plt.xlabel("Work load Average/day ")
plt.title("BoxPlot of Variable 'Work load Average/day '")
plt.ylabel('Values')

Text(0, 0.5, 'Values')

# Let us plot Height
plt.boxplot(df['Height'])
plt.xlabel("Height")
plt.title("BoxPlot of Variable for 'Height'")
plt.ylabel('Values')

#calculate remaining ones
plt.boxplot([ df['Distance from Residence to Work'], df['Service time'], df['Age'], df['Hit target'], df['Weight'], df['Body mass index']])
plt.xlabel(['1. Distance from Residence to Work', '2. Service time', '3. Age', '4. Hit target', '5. Weight', '6. Body mass index'])
plt.title("BoxPlot of rest of the Variables")
plt.ylabel('Values')

# From the above mentioned boxplot, we can clearly see that in variables 'Distance from Residence to Work', 'Weight' and 'Body mass index'
## so, there is no outlier
### see the attached graph of output

# Let us now list variables which doesn't have outlier
neglect = ['Distance from Residence to Work', 'Weight', 'Body mass index']

#Let us now use the loops
## Looping over all continuou variables to detect and remove Outliers
for i in continuous_variables:
## Avoiding the variables which doesn't have outlier
    if i in neglect:
        continue
# Getting 75 and 25 percentile of variable "i"
    q75, q25 = np.percentile(df[i], [75,25])
# Calculating Interquartile range
    iqr = q75 - q25
    
# Calculating upper extream and lower extream
    minimum = q25 - (iqr*1.5)
    maximum = q75 + (iqr*1.5)
    
# Replacing all the outliers value to NA
    df.loc[df[i]< minimum,i] = np.nan
    df.loc[df[i]> maximum,i] = np.nan

# Now let us again impute missing values with KNN
df = pd.DataFrame(KNN(k = 3).fit_transform(df), columns = df.columns)
# Checking if there is any missing value
df.isnull().sum().sum()

Feature Selection

## Let us now see the Correlation analysis for continuous variables
#Correlation plot
df_corr = df.loc[:,continuous_variables]

# Let us now set the width and hieght of the plot
f, ax = plt.subplots(figsize=(10, 10))

#Generate correlation matrix
corr = df_corr.corr()

#Plot using seaborn library
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), 
            cmap=sns.diverging_palette(220, 50, as_cmap=True),
            square=True, ax=ax, annot = True)
plt.plot()

# Let us loop for ANOVA test - Since the target variable is continuous
for i in categorical_variables:
    f, p = stats.f_oneway(df[i], df["Absenteeism time in hours"])
    print("P value for variable "+str(i)+" is "+str(p))

# Let us drop the variables which has redundant information
to_drop = ['Weight']
df = df.drop(to_drop, axis = 1)

# Let us put the Continuous Variables and Categorical Variables after droping variables
continuous_variables = [i for i in continuous_variables if i not in to_drop]
categorical_variables = [i for i in categorical_variables if i not in to_drop]

# For the cleaning of data let us just make a copy
clean_data = df.copy()

# Let us now check if there is any normally distributed variable in data
for i in continuous_variables:
    if i == 'Absenteeism time in hours':
        continue
    sns.distplot(df[i],bins = 'auto')
    plt.title("Checking Distribution for Variable "+str(i))
    plt.ylabel("Density")
    plt.show()

# Let us now check if there is any normally distributed variable in data
# Since we will not find the normally distributed curve for this dataset, we will use Normalizationg for Feature Scalling
##Normalization
for i in continuous_variables:
    if i == 'Absenteeism time in hours':
        continue
    df[i] = (df[i] - df[i].min())/(df[i].max()-df[i].min())

Machine Laearning

# Will get the dummy variables for categorical variables
df = pd.get_dummies(data = df, columns = categorical_variables)

## Copying the dataframe
df1 = df.copy()

# Will see the data
df.iloc[:,8].head()

# Will see the 1st row of data
df.iloc[:, df.columns != 'Absenteeism time in hours'].head(1)

# Now coming to train_test. Using train_test_split sampling function for test and train data split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split( df.iloc[:, df.columns != 'Absenteeism time in hours'], df.iloc[:, 8], test_size = 0.20)

Decision Tree

# Let us now Import libraries for Decision Tree 
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Now will build model on top of training dataset
fit_DT = DecisionTreeRegressor(max_depth = 2).fit(X_train,y_train)

# Will calculate RMSE for training data to check for over fitting
pred_train = fit_DT.predict(X_train)
rmse_for_train = np.sqrt(mean_squared_error(y_train,pred_train))

# Will Calculate RMSE for test data to check accuracy
pred_test = fit_DT.predict(X_test)
rmse_for_test =np.sqrt(mean_squared_error(y_test,pred_test))

print("Root Mean Squared Error For Training data = "+str(rmse_for_train))
print("Root Mean Squared Error For Test data = "+str(rmse_for_test))
print("R^2 Score(coefficient of determination) = "+str(r2_score(y_test,pred_test)))

Random Forest

# Let us now Import libraries for Decision Tree 
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error

# Now will build model on top of training dataset
fit_DT = DecisionTreeRegressor(max_depth = 2).fit(X_train,y_train)

# Will calculate RMSE for training data to check for over fitting
pred_train = fit_DT.predict(X_train)
rmse_for_train = np.sqrt(mean_squared_error(y_train,pred_train))

# Will Calculate RMSE for test data to check accuracy
pred_test = fit_DT.predict(X_test)
rmse_for_test =np.sqrt(mean_squared_error(y_test,pred_test))

print("Root Mean Squared Error For Training data = "+str(rmse_for_train))
print("Root Mean Squared Error For Test data = "+str(rmse_for_test))
print("R^2 Score(coefficient of determination) = "+str(r2_score(y_test,pred_test)))

Linear Regression

# Let us import libraries for Linear Regression
from sklearn.linear_model import LinearRegression

# Building model on top of training dataset
fit_LR = LinearRegression().fit(X_train , y_train)

# Calculating RMSE for training data to check for over fitting
pred_train = fit_LR.predict(X_train)
rmse_for_train = np.sqrt(mean_squared_error(y_train,pred_train))

# Calculating RMSE for test data to check accuracy
pred_test = fit_LR.predict(X_test)
rmse_for_test =np.sqrt(mean_squared_error(y_test,pred_test))

print("Root Mean Squared Error For Training data = "+str(rmse_for_train))
print("Root Mean Squared Error For Test data = "+str(rmse_for_test))
print("R^2 Score(coefficient of determination) = "+str(r2_score(y_test,pred_test)))



